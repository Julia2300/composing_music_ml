{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e22ff8f9-cea1-42a2-8792-31b451e99e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from model_functions import PaddedDataset, trainer_gpt2_transformer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "PATH_VOCAB = \"../0_data/5_vocabs\"\n",
    "PATH_WORD_DATA = \"../0_data/6_word_data\"\n",
    "PATH_MODELS = \"../0_data/7_models\"\n",
    "PATH_MODELS_LOSS = \"../0_data/7_models/loss\"\n",
    "PATH_MODELS_CONFIG = \"../0_data/7_models/config\"\n",
    "\n",
    "for path in [PATH_MODELS, PATH_MODELS_LOSS, PATH_MODELS_CONFIG]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f9159c4-96f5-4ec0-b354-ca07f7c23b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5acaadfc-996f-4ea9-acb6-21a371519b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>max_length</th>\n",
       "      <th>emb_dim</th>\n",
       "      <th>attention_heads</th>\n",
       "      <th>layers</th>\n",
       "      <th>dropout</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>ran</th>\n",
       "      <th>runtime</th>\n",
       "      <th>runtime_min</th>\n",
       "      <th>min_loss</th>\n",
       "      <th>at_epoch</th>\n",
       "      <th>incorrect_notes</th>\n",
       "      <th>correct_notes</th>\n",
       "      <th>correct_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>13.0950</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.166360</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a2</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>12.5940</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2.140638</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>69.6</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>12.4513</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2.050499</td>\n",
       "      <td>2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>79.2</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>11.8954</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.046405</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.4</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>12.1114</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.138924</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>80.4</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>d</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>11.6138</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.815254</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71.8</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name  max_length  emb_dim  attention_heads  layers  dropout  learning_rate   \n",
       "0   a1         256      256                4       6     0.01          0.001  \\\n",
       "1   a2         256      256                4       6     0.01          0.001   \n",
       "2   a3         256      256                4       6     0.01          0.001   \n",
       "3    b         256      256                4       6     0.01          0.001   \n",
       "4    c         256      256                4       6     0.01          0.001   \n",
       "5    d         256      256                4       6     0.01          0.001   \n",
       "\n",
       "   epochs  batch_size  ran  runtime  runtime_min  min_loss  at_epoch   \n",
       "0       2           4  yes  13.0950         0.22  2.166360         2  \\\n",
       "1       2           4  yes  12.5940         0.21  2.140638         2   \n",
       "2       2           4  yes  12.4513         0.21  2.050499         2   \n",
       "3       2           4  yes  11.8954         0.20  2.046405         2   \n",
       "4       2           4  yes  12.1114         0.20  2.138924         2   \n",
       "5       2           4  yes  11.6138         0.19  1.815254         2   \n",
       "\n",
       "   incorrect_notes  correct_notes  correct_rate  \n",
       "0              0.6           81.0          0.99  \n",
       "1              0.8           69.6          0.99  \n",
       "2              1.4           79.2          0.98  \n",
       "3              0.0           80.4          1.00  \n",
       "4              0.2           80.4          1.00  \n",
       "5              1.0           71.8          0.99  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df = pd.read_excel(f\"{PATH_MODELS}/comparison_model_stats.xlsx\", index_col=\"Unnamed: 0\")\n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78af6ff2-0eec-4c6c-a640-d48ecce26f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>max_length</th>\n",
       "      <th>emb_dim</th>\n",
       "      <th>attention_heads</th>\n",
       "      <th>layers</th>\n",
       "      <th>dropout</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>ran</th>\n",
       "      <th>runtime</th>\n",
       "      <th>runtime_min</th>\n",
       "      <th>min_loss</th>\n",
       "      <th>at_epoch</th>\n",
       "      <th>incorrect_notes</th>\n",
       "      <th>correct_notes</th>\n",
       "      <th>correct_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a1</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>13.0950</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.166360</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a2</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>12.5940</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2.140638</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>69.6</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a3</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>12.4513</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2.050499</td>\n",
       "      <td>2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>79.2</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>11.8954</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.046405</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.4</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>12.1114</td>\n",
       "      <td>0.20</td>\n",
       "      <td>2.138924</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>80.4</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>d</td>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>11.6138</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.815254</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71.8</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name  max_length  emb_dim  attention_heads  layers  dropout  learning_rate   \n",
       "0   a1         256      256                4       6     0.01          0.001  \\\n",
       "1   a2         256      256                4       6     0.01          0.001   \n",
       "2   a3         256      256                4       6     0.01          0.001   \n",
       "3    b         256      256                4       6     0.01          0.001   \n",
       "4    c         256      256                4       6     0.01          0.001   \n",
       "5    d         256      256                4       6     0.01          0.001   \n",
       "\n",
       "   epochs  batch_size  ran  runtime  runtime_min  min_loss  at_epoch   \n",
       "0       2           4  yes  13.0950         0.22  2.166360         2  \\\n",
       "1       2           4  yes  12.5940         0.21  2.140638         2   \n",
       "2       2           4  yes  12.4513         0.21  2.050499         2   \n",
       "3       2           4  yes  11.8954         0.20  2.046405         2   \n",
       "4       2           4  yes  12.1114         0.20  2.138924         2   \n",
       "5       2           4  yes  11.6138         0.19  1.815254         2   \n",
       "\n",
       "   incorrect_notes  correct_notes  correct_rate  \n",
       "0              0.6           81.0          0.99  \n",
       "1              0.8           69.6          0.99  \n",
       "2              1.4           79.2          0.98  \n",
       "3              0.0           80.4          1.00  \n",
       "4              0.2           80.4          1.00  \n",
       "5              1.0           71.8          0.99  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "for index, row in model_df.iterrows():\n",
    "    \n",
    "    # only run models that not ran yet\n",
    "    if row[\"ran\"] == \"yes\":\n",
    "        continue\n",
    "    \n",
    "    # create model name directories\n",
    "    model_name = row[\"name\"]\n",
    "    model_dirs = {\n",
    "        \"loss\": f\"{PATH_MODELS_LOSS}/{model_name}\",\n",
    "        \"out\": f\"{PATH_MODELS_CONFIG}/{model_name}\"\n",
    "    }\n",
    "    for key in model_dirs:\n",
    "        if not os.path.exists(model_dirs[key]):\n",
    "            os.makedirs(model_dirs[key])\n",
    "    \n",
    "    # save hyperparameters as dictionary\n",
    "    model_hyperparameters = {\n",
    "        \"max_length\": row[\"max_length\"],\n",
    "        \"emb_dim\": row[\"emb_dim\"],\n",
    "        \"attention_heads\": row[\"attention_heads\"],\n",
    "        \"layers\": row[\"layers\"],\n",
    "        \"dropout\": row[\"dropout\"],\n",
    "        \"learning_rate\": row[\"learning_rate\"],\n",
    "        \"epochs\": row[\"epochs\"],\n",
    "        \"batch_size\": row[\"batch_size\"],\n",
    "    }\n",
    "    \n",
    "    # create tokenizer\n",
    "    tokenizer = GPT2Tokenizer(\n",
    "        vocab_file=f\"{PATH_VOCAB}/vocab_{model_name}.json\", \n",
    "        merges_file=f\"{PATH_VOCAB}/merges.txt\")\n",
    "    tokenizer.add_special_tokens({'pad_token': 'PAD', 'bos_token': 'BOS', 'eos_token': 'EOS',})\n",
    "    \n",
    "    # read in data\n",
    "    with open(f\"{PATH_WORD_DATA}/{model_name}_data.json\", 'r') as fp:\n",
    "        json_data = json.load(fp)\n",
    "    song_list = []\n",
    "    for song in json_data:\n",
    "        song_list.append(json_data[song])\n",
    "    data = [\" \".join(song) for song in song_list]\n",
    "    split_train_test = int(0.9*len(data))\n",
    "    \n",
    "    # create datasets and define data collator\n",
    "    train_dataset = PaddedDataset(tokenizer=tokenizer, data=data[:split_train_test], max_length=row[\"max_length\"])\n",
    "    eval_dataset = PaddedDataset(tokenizer=tokenizer, data=data[split_train_test:], max_length=row[\"max_length\"])\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    model_data = {\n",
    "        \"train_dataset\": train_dataset,\n",
    "        \"eval_dataset\": eval_dataset,\n",
    "        \"data_collator\": data_collator,\n",
    "    }\n",
    "    \n",
    "    # create and train model trainer\n",
    "    trainer = trainer_gpt2_transformer(\n",
    "        hyperparameters = model_hyperparameters,\n",
    "        tokenizer = tokenizer,\n",
    "        data = model_data,\n",
    "        dirs = model_dirs,\n",
    "    )\n",
    "    trainer.train()\n",
    "    \n",
    "    # save runtime and model loss\n",
    "    log_hist = trainer.state.log_history\n",
    "    eval_loss = [log_hist[i][\"eval_loss\"] for i in range(1,len(log_hist),2)]\n",
    "    train_loss = [log_hist[i][\"loss\"] for i in range(0,len(log_hist)-1,2)]\n",
    "    runtime = log_hist[-1][\"train_runtime\"]\n",
    "    \n",
    "    model_df.at[index,\"runtime\"] = runtime\n",
    "    model_df.at[index,\"runtime_min\"] = (runtime/60).__round__(2)\n",
    "    model_df.at[index,\"min_loss\"] = min(eval_loss)\n",
    "    model_df.at[index,\"at_epoch\"] = np.argmin(eval_loss) + 1\n",
    "\n",
    "    # show loss plot\n",
    "    plt.plot(train_loss, color=\"blue\")\n",
    "    plt.plot(eval_loss, color=\"orange\")\n",
    "    plt.savefig(f\"{model_dirs['loss']}/loss_graph_{model_name}.jpg\")\n",
    "    plt.show()\n",
    "    \n",
    "    # save model and set ran to yes\n",
    "    trainer.save_model(f\"{model_dirs['out']}/end_version\")\n",
    "    model_df.at[index,\"ran\"] = \"yes\"\n",
    "    \n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84f0b777-7f33-4240-a67b-2ad024a8ac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.to_excel(f\"{PATH_MODELS}/comparison_model_stats.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [pipenv: PyEnv]",
   "language": "python",
   "name": "pyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
