{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bar_None',\n",
       " 'Note-On_60',\n",
       " 'Note-On_61',\n",
       " 'Note-On_62',\n",
       " 'Note-On_63',\n",
       " 'Note-On_64',\n",
       " 'Note-On_65',\n",
       " 'Note-On_66',\n",
       " 'Note-On_67',\n",
       " 'Note-On_68']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('words_to_tokens.json', 'r') as fp:\n",
    "    tokens = json.load(fp)\n",
    "\n",
    "list(tokens.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bar_None': 0,\n",
       " 'Note-On_60': 1,\n",
       " 'Note-On_61': 2,\n",
       " 'Note-On_62': 3,\n",
       " 'Note-On_63': 4,\n",
       " 'Note-On_64': 5,\n",
       " 'Note-On_65': 6,\n",
       " 'Note-On_66': 7,\n",
       " 'Note-On_67': 8,\n",
       " 'Note-On_68': 9,\n",
       " 'Note-On_69': 10,\n",
       " 'Note-On_70': 11,\n",
       " 'Note-On_71': 12,\n",
       " 'Note-On_72': 13,\n",
       " 'Note-On_73': 14,\n",
       " 'Note-On_74': 15,\n",
       " 'Note-On_75': 16,\n",
       " 'Note-On_76': 17,\n",
       " 'Note-On_77': 18,\n",
       " 'Note-On_78': 19,\n",
       " 'Note-On_79': 20,\n",
       " 'Note-On_80': 21,\n",
       " 'Note-On_81': 22,\n",
       " 'Note-On_82': 23,\n",
       " 'Note-On_83': 24,\n",
       " 'Note-On_84': 25,\n",
       " 'Note-On_85': 26,\n",
       " 'Note-On_86': 27,\n",
       " 'Note-On_87': 28,\n",
       " 'Note-On_88': 29,\n",
       " 'Note-On_89': 30,\n",
       " 'Note-On_90': 31,\n",
       " 'Note-On_91': 32,\n",
       " 'Note-On_92': 33,\n",
       " 'Note-On_93': 34,\n",
       " 'Note-On_94': 35,\n",
       " 'Note-On_95': 36,\n",
       " 'Note-Duration_1': 37,\n",
       " 'Note-Duration_2': 38,\n",
       " 'Note-Duration_3': 39,\n",
       " 'Note-Duration_4': 40,\n",
       " 'Note-Duration_5': 41,\n",
       " 'Note-Duration_6': 42,\n",
       " 'Note-Duration_7': 43,\n",
       " 'Note-Duration_8': 44,\n",
       " 'Note-Duration_9': 45,\n",
       " 'Note-Duration_10': 46,\n",
       " 'Note-Duration_11': 47,\n",
       " 'Note-Duration_12': 48,\n",
       " 'Note-Duration_13': 49,\n",
       " 'Note-Duration_14': 50,\n",
       " 'Note-Duration_15': 51,\n",
       " 'Note-Duration_16': 52,\n",
       " 'Note-Duration_17': 53,\n",
       " 'Note-Duration_18': 54,\n",
       " 'Note-Duration_19': 55,\n",
       " 'Note-Duration_20': 56,\n",
       " 'Note-Duration_21': 57,\n",
       " 'Note-Duration_22': 58,\n",
       " 'Note-Duration_23': 59,\n",
       " 'Note-Duration_24': 60,\n",
       " 'Note-Duration_25': 61,\n",
       " 'Note-Duration_26': 62,\n",
       " 'Note-Duration_27': 63,\n",
       " 'Note-Duration_28': 64,\n",
       " 'Note-Duration_29': 65,\n",
       " 'Note-Duration_30': 66,\n",
       " 'Note-Duration_31': 67,\n",
       " 'Note-Duration_32': 68,\n",
       " 'Note-Duration_33': 69,\n",
       " 'Note-Duration_34': 70,\n",
       " 'Note-Duration_35': 71,\n",
       " 'Note-Duration_36': 72,\n",
       " 'Note-Duration_37': 73,\n",
       " 'Note-Duration_38': 74,\n",
       " 'Note-Duration_39': 75,\n",
       " 'Note-Duration_40': 76,\n",
       " 'Note-Duration_41': 77,\n",
       " 'Note-Duration_42': 78,\n",
       " 'Note-Duration_43': 79,\n",
       " 'Note-Duration_44': 80,\n",
       " 'Note-Duration_45': 81,\n",
       " 'Note-Duration_46': 82,\n",
       " 'Note-Duration_47': 83,\n",
       " 'Note-Duration_48': 84,\n",
       " 'Note-Duration_49': 85,\n",
       " 'Note-Duration_50': 86,\n",
       " 'Note-Duration_51': 87,\n",
       " 'Note-Duration_52': 88,\n",
       " 'Note-Duration_53': 89,\n",
       " 'Note-Duration_54': 90,\n",
       " 'Note-Duration_55': 91,\n",
       " 'Note-Duration_56': 92,\n",
       " 'Note-Duration_57': 93,\n",
       " 'Note-Duration_58': 94,\n",
       " 'Note-Duration_59': 95,\n",
       " 'Note-Duration_60': 96,\n",
       " 'Note-Duration_61': 97,\n",
       " 'Note-Duration_62': 98,\n",
       " 'Note-Duration_63': 99,\n",
       " 'Note-Duration_64': 100,\n",
       " 'Note-Duration_triole': 101,\n",
       " 'Position_1/16': 102,\n",
       " 'Position_2/16': 103,\n",
       " 'Position_3/16': 104,\n",
       " 'Position_4/16': 105,\n",
       " 'Position_5/16': 106,\n",
       " 'Position_6/16': 107,\n",
       " 'Position_7/16': 108,\n",
       " 'Position_8/16': 109,\n",
       " 'Position_9/16': 110,\n",
       " 'Position_10/16': 111,\n",
       " 'Position_11/16': 112,\n",
       " 'Position_12/16': 113,\n",
       " 'Position_13/16': 114,\n",
       " 'Position_14/16': 115,\n",
       " 'Position_15/16': 116,\n",
       " 'Position_16/16': 117,\n",
       " 'Position-Triole_1': 118,\n",
       " 'Position-Triole_2': 119}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokens[\"[UNK]\"] = 120\n",
    "#tokens[\"[CLS]\"] = 121\n",
    "#tokens[\"[SEP]\"] = 122\n",
    "#tokens[\"[PAD]\"] = 123\n",
    "#tokens[\"[MASK]\"] = 124\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocab.txt\", \"w\") as txt_file:\n",
    "    for word in list(tokens.keys()) + [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]:\n",
    "        txt_file.write(\"\".join(word) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.json', 'w') as fp:\n",
    "    json.dump(tokens, fp, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "803"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data_words.json', 'r') as fp:\n",
    "    data = json.load(fp)\n",
    "\n",
    "song_list = []\n",
    "for song in data:\n",
    "    song_list.append(data[song])\n",
    "\n",
    "len(song_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "803"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data.json', 'r') as fp:\n",
    "    data = json.load(fp)\n",
    "\n",
    "token_list = []\n",
    "for song in data:\n",
    "    token_list.append(data[song])\n",
    "\n",
    "len(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bar_None Position_3/16 Note-On_76 Note-Duration_2 Position_4/16 Note-On_74 Note-Duration_2 Position_'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "as_sentences = [\" \".join(song) for song in song_list]\n",
    "as_sentences[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output.txt\", \"w\", encoding='utf-8') as txt_file:\n",
    "    for line in as_sentences[:-1]:\n",
    "        txt_file.write(\"\".join(line) + \"\\n\")\n",
    "    txt_file.write(\"\".join(as_sentences[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test Tokenizer variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### BPE und Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "tokenizer.add_tokens(list(tokens.keys()))\n",
    "tokenizer.add_special_tokens([\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=1352, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "\n",
    "output = tokenizer.encode(as_sentences[1], add_special_tokens=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 112, 5, 38, 113, 5, 38, 114, 5, 38, 116, 5, 38, 117, 5, 38, 0, 102, 5, 38]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.ids[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.id_to_token(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.enable_padding(pad_id=126, pad_token=\"[PAD]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2LMHeadModel,PreTrainedTokenizer\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define your own vocabulary\n",
    "vocabulary = [\"this\", \"is\", \"sentence\", \"1\", \"2\", \"3\", \".\"]\n",
    "\n",
    "# Create a custom tokenizer by inheriting from PreTrainedTokenizer\n",
    "class CustomTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        #self.vocab_size = len(vocab)\n",
    "        self.pad_token = \"<pad>\"\n",
    "        self.eos_token = \"<eos>\"\n",
    "        self.unk_token = \"<unk>\"\n",
    "        self.cls_token = \"<cls>\"\n",
    "        self.mask_token = \"<mask>\"\n",
    "\n",
    "        self.vocab_dict = {v: i for i, v in enumerate(self.vocab)}\n",
    "\n",
    "    def __call__(self, text):\n",
    "        # Implement tokenization logic here\n",
    "        tokens = text.split()\n",
    "        return tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        # Implement conversion from tokens to ids here\n",
    "        return [self.vocab_dict[t] for t in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        # Implement conversion from ids to tokens here\n",
    "        return [self.vocab[i] for i in ids]\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = self(text)\n",
    "        ids = self.convert_tokens_to_ids(tokens)\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        tokens = self.convert_ids_to_tokens(ids)\n",
    "        text = \" \".join(tokens)\n",
    "        return text\n",
    "\n",
    "# Create an instance of the custom tokenizer with your vocabulary\n",
    "tokenizer = CustomTokenizer(vocabulary)\n",
    "\n",
    "# Define your GPT-2 model configuration\n",
    "model_config = GPT2Config(vocab_size=len(vocabulary))\n",
    "\n",
    "# Create your GPT-2 model\n",
    "model = GPT2LMHeadModel(config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### pretrained gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", do_lower_case=False)\n",
    "#tokenizer(\"Hello world\")['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test Data Variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test data via dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(songs_test, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bar P1 N61 D3 P2 N62 D2 P3 N63 D3 Bar', 'Bar P1 N60 D3 P2 N62 D3 P3 N63 D2 Bar', 'Bar P1 N60 D1 P2 N62 D2 P3 N63 D2 Bar', 'Bar P1 N62 D2 P2 N62 D1 P3 N63 D1 Bar']\n",
      "['Bar P1 N61 D2 P2 N60 D3 P3 N60 D2 Bar', 'Bar P1 N62 D1 P2 N62 D3 P3 N63 D3 Bar', 'Bar P1 N60 D2 P2 N60 D1 P3 N60 D1 Bar', 'Bar P1 N60 D1 P2 N60 D1 P3 N60 D1 Bar']\n"
     ]
    }
   ],
   "source": [
    "for i in iter(train_dataloader):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data via dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bar P1 N60 D1 P2 N60 D1 P3 N60 D1 Bar\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "songs_test = [ \n",
    "    \"Bar P1 N60 D1 P2 N60 D1 P3 N60 D1 Bar\",\n",
    "    \"Bar P1 N62 D2 P2 N62 D1 P3 N61 D1 Bar\",\n",
    "    \"Bar P1 N61 D3 P2 N62 D2 P3 N62 D3 Bar\",\n",
    "    \"Bar P1 N62 D1 P2 N62 D3 P3 N61 D3 Bar\",\n",
    "    \"Bar P1 N61 D2 P2 N60 D3 P3 N60 D2 Bar\",\n",
    "    \"Bar P1 N60 D3 P2 N62 D3 P3 N62 D2 Bar\",\n",
    "    \"Bar P1 N60 D1 P2 N62 D2 P3 N61 D2 Bar\",\n",
    "    \"Bar P1 N60 D2 P2 N60 D1 P3 N60 D1 Bar\"\n",
    "]\n",
    "test_train_dataset = StringListDataset(songs_test)\n",
    "\n",
    "print(test_train_dataset[0])\n",
    "print(len(test_train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LineByLineDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess your training data\n",
    "#train_dataset = LineByLineTextDataset(\n",
    "#    tokenizer=tokenizer,\n",
    "#    file_path=\"output3.txt\",\n",
    "#    block_size=4\n",
    "#)\n",
    "#\n",
    "#data_collator = DataCollatorForLanguageModeling(\n",
    "#    tokenizer=tokenizer, mlm=False\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GPT Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test data via files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_test = {\n",
    "    \"Bar\": 0,\n",
    "    \"N60\": 1,\n",
    "    \"N61\": 2,\n",
    "    \"N62\": 3,\n",
    "    \"D1\": 4,\n",
    "    \"D2\": 5,\n",
    "    \"D3\": 6,\n",
    "    \"P1\": 7,\n",
    "    \"P2\": 8,\n",
    "    \"P3\": 9,\n",
    "    #\"[UNK]\": 10,\n",
    "    #\"[CLS]\": 11,\n",
    "    #\"[SEP]\": 12,\n",
    "    \"PAD\": 10,\n",
    "    #\"[MASK]\": 14\n",
    "}\n",
    "\n",
    "with open('vocab3.json', 'w') as fp:\n",
    "    json.dump(tokens_test, fp, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_test = [ \n",
    "    \"Bar P1 N60 D1 P2 N60 D1 P3 N60 D1 Bar\",\n",
    "    \"Bar P1 N62 D2 P2 N62 D1 P3 N63 D1 Bar\",\n",
    "    \"Bar P1 N61 D3 P2 N62 D2 P3 N63 D3 Bar\",\n",
    "    \"Bar P1 N62 D1 P2 N62 D3 P3 N63 D3 Bar\",\n",
    "    \"Bar P1 N61 D2 P2 N60 D3 P3 N60 D2 Bar\",\n",
    "    \"Bar P1 N60 D3 P2 N62 D3 P3 N63 D2 Bar\",\n",
    "    \"Bar P1 N60 D1 P2 N62 D2 P3 N63 D2 Bar\",\n",
    "    \"Bar P1 N60 D2 P2 N60 D1 P3 N60 D1 Bar\"\n",
    "]\n",
    "\n",
    "with open(\"output3.txt\", \"w\", encoding='utf-8') as txt_file:\n",
    "    for line in songs_test[:-1]:\n",
    "        txt_file.write(\"\".join(line) + \"\\n\")\n",
    "    txt_file.write(\"\".join(songs_test[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#device = \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GPT2 tokenizer\n",
    "#tokenizer = GPT2Tokenizer(\n",
    "#    vocab_file=\"vocab3.json\", \n",
    "#    merges_file=\"merges3.txt\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer(\n",
    "    vocab_file=\"vocab.json\", \n",
    "    merges_file=\"merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': 'PAD'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data length: 803\n",
      "90% at:      722\n"
     ]
    }
   ],
   "source": [
    "split_train_test = int(0.9*len(as_sentences))\n",
    "print(\"data length:\", len(as_sentences))\n",
    "print(\"90% at:     \", split_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_data = [ \n",
    "    \"Bar P1 N60 D1 P2 N60 D1 P3 N60 D1 Bar\",\n",
    "    \"Bar P1 N62 D2 P2 N62 D1 P3 N61 D1 Bar\",\n",
    "    \"Bar P1 N61 D3 P2 N62 D2 P3 N62 D3 Bar\",\n",
    "    \"Bar P1 N62 D1 P2 N62 D3 P3 N61 D3 Bar\",\n",
    "    \"Bar P1 N61 D2 P2 N60 D3 P3 N60 D2 Bar\",\n",
    "    \"Bar P1 N60 D3 P2 N62\",\n",
    "    \"Bar P1 N60 D1 P2 N62 D2 P3 N61 D2 Bar\",\n",
    "    \"Bar P1 N60 D2 P2 N60 D1 P3 N60\"\n",
    "]\n",
    "\"\"\"\n",
    "train_data = as_sentences[:split_train_test]\n",
    "eval_data = as_sentences[split_train_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def pad_and_encode_data(data, max_length=128, padding_token=\"[PAD]\"):\n",
    "#    encoded_data = []\n",
    "#    for song in data:\n",
    "#        song = song.split(\" \")\n",
    "#        if len(song) < max_length:\n",
    "#            remaining_length = max_length - len(song)\n",
    "#            song = song + [padding_token] * remaining_length\n",
    "#        else:\n",
    "#            song = song[:max_length]\n",
    "#        encoded_data.append(song)\n",
    "#    \n",
    "#    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = pad_and_encode_data(train_data, max_length=12)\n",
    "#train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetNew(Dataset):\n",
    "    def __init__(self, tokenizer, data, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            self.data[index].split(\" \"),\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100  # Set padding tokens to -100 for language modeling\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your custom Dataset\n",
    "train_dataset = CustomDatasetNew(tokenizer=tokenizer, data=train_data, max_length=32)\n",
    "eval_dataset = CustomDatasetNew(tokenizer=tokenizer, data=eval_data, max_length=32)\n",
    "\n",
    "# Define your data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  0, 107, 118,   5,  38, 109,   8,  38, 111, 118,  10,  38, 113,  13,\n",
       "           44, 101, 115, 118,  12,  52, 101,   0, 103, 118,  10,  38, 105,   8,\n",
       "           38, 107, 118,  10]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'labels': tensor([[  0, 107, 118,   5,  38, 109,   8,  38, 111, 118,  10,  38, 113,  13,\n",
       "           44, 101, 115, 118,  12,  52, 101,   0, 103, 118,  10,  38, 105,   8,\n",
       "           38, 107, 118,  10]])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GPT-2 model architecture\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=512, # max seq length\n",
    "    n_embd=32,\n",
    "    n_head=2, \n",
    "    n_layer=3,\n",
    "    dropout=0.1 \n",
    ")\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"out\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=4, # You can adjust the batch size per device as needed\n",
    "    save_steps=1000,\n",
    "    save_total_limit=5, # maximum number of models to save\n",
    "    learning_rate=1e-4, # You can adjust the learning rate as needed\n",
    "    #weight_decay=0.01, # You can adjust the weight decay as needed\n",
    "    #warmup_steps=1_000, # Number of warmup steps for learning rate scheduling\n",
    "    logging_dir='logs', # Directory to save the training logs\n",
    "    logging_steps=100, # Number of steps to log training progress\n",
    "    seed=4711, # Set a seed for reproducibility\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Create and train  Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1810' max='1810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1810/1810 00:33, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.388100</td>\n",
       "      <td>4.111854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.915000</td>\n",
       "      <td>3.767926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.625300</td>\n",
       "      <td>3.520708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.401700</td>\n",
       "      <td>3.324670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.227500</td>\n",
       "      <td>3.173990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.092800</td>\n",
       "      <td>3.062072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.996800</td>\n",
       "      <td>2.983178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.930400</td>\n",
       "      <td>2.928416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.886300</td>\n",
       "      <td>2.898138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.866500</td>\n",
       "      <td>2.888235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "training = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1810, training_loss=3.33303306958952, metrics={'train_runtime': 33.4721, 'train_samples_per_second': 215.702, 'train_steps_per_second': 54.075, 'total_flos': 52921098240.0, 'train_loss': 3.33303306958952, 'epoch': 10.0})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"Bar_None\".split(\" \"), return_tensors=\"pt\")\n",
    "inputs = inputs.to(device)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Variante A ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 120])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(inputs)\n",
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8728, -0.9974, -1.3476, -0.8551, -1.4070, -0.4868, -0.9296,\n",
       "          -1.4357, -0.2433, -1.2373, -0.3960, -1.4038, -0.5088, -0.1439,\n",
       "          -1.3268, -0.2886, -1.2116, -0.2242, -0.9016, -1.1739, -0.6277,\n",
       "          -1.2017, -0.5254, -1.3868, -1.0088, -0.9413, -1.2695, -1.1574,\n",
       "          -1.2903, -1.0802, -1.3319, -1.1735, -1.2646, -1.4013, -1.1021,\n",
       "          -1.2026, -1.1902, -1.2390,  0.6239, -1.4337,  0.5129, -1.3960,\n",
       "          -0.2931, -1.3144,  0.4770, -1.1552, -0.8063, -1.3491, -0.7926,\n",
       "          -1.1014, -1.1950, -1.3592, -0.3322, -1.3793, -1.2492, -1.5371,\n",
       "          -1.2450, -1.2865, -1.4448, -1.2782, -1.1670, -1.4160, -1.1576,\n",
       "          -1.3678, -1.2293, -1.1111, -1.0761, -1.2141, -0.8574, -1.3414,\n",
       "          -1.3950, -1.2461, -1.1994, -1.3287, -1.2462, -1.1890, -1.0250,\n",
       "          -1.3294, -1.4661, -1.4220, -1.4358, -1.2183, -1.1498, -1.3585,\n",
       "          -1.2873, -1.5328, -1.1576, -1.3172, -1.2318, -1.4232, -1.3945,\n",
       "          -1.3108, -1.0180, -1.2760, -1.4022, -1.2348, -1.1658, -1.3339,\n",
       "          -1.2157, -1.3528, -1.2585,  0.9712,  1.3637,  1.8341,  1.5956,\n",
       "           1.0486,  1.3050,  1.5419,  1.5301,  1.0466,  1.4520,  1.5567,\n",
       "           1.5635,  1.0119,  1.1875,  1.5411,  1.4541,  0.9149,  0.3843,\n",
       "           0.2729]]], device='cuda:0', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[23]], device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Temperature value\n",
    "temperature = 13\n",
    "\n",
    "# Convert logits to probabilities using softmax with temperature\n",
    "probs = F.softmax(outputs.logits / temperature, dim=-1)\n",
    "\n",
    "# Sample a token from the probability distribution for each position in the sequence\n",
    "predicted_tokens = torch.multinomial(probs.view(-1, probs.shape[-1]), num_samples=1).view(*probs.shape[:-1])\n",
    "predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Note-On_82'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(predicted_tokens[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Variante B ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0, 103,  17,  38,   0, 103,  17,  38,   0, 103,  17,  38,   0,\n",
       "         103,  17,  38,   0, 103,  17]], device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#outputs = model.generate(inputs, max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
    "outputs = model.generate(inputs, max_length=20, temperature=0.8)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bar_NoneBar_NonePosition_2/16Note-On_76Note-Duration_2Bar_NonePosition_2/16Note-On_76Note-Duration_2Bar_NonePosition_2/16Note-On_76Note-Duration_2Bar_NonePosition_2/16Note-On_76Note-Duration_2Bar_NonePosition_2/16Note-On_76'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Old version Dataset #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data[index]\n",
    "        # Tokenize the text\n",
    "        input_ids = self.tokenizer.encode(text.split(\" \"), add_special_tokens=True)\n",
    "        return torch.tensor(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the training data using your custom tokenizer\n",
    "train_dataset = CustomDataset(tokenizer, train_data)\n",
    "\n",
    "# Create a data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### ALTERNATIVE PARAMETERS?? ##########\n",
    "\n",
    "# Define the GPT2 model configuration\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_positions=512, # You can adjust the maximum sequence length as needed\n",
    "    n_ctx=512,\n",
    "    n_embd=768, # You can adjust the model dimension as needed\n",
    "    n_layer=12, # You can adjust the number of layers as needed\n",
    "    n_head=12, # You can adjust the number of attention heads as needed\n",
    "    dropout=0.1 # You can adjust the dropout rate as needed\n",
    ")\n",
    "\n",
    "# Instantiate the GPT2 model\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='out', # Directory to save the model and training logs\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10, # You can adjust the number of epochs as needed\n",
    "    per_device_train_batch_size=4, # You can adjust the batch size per device as needed\n",
    "    save_steps=10_000, # Number of steps to save the model during training\n",
    "    save_total_limit=2, # Number of saved models to keep\n",
    "    learning_rate=1e-4, # You can adjust the learning rate as needed\n",
    "    weight_decay=0.01, # You can adjust the weight decay as needed\n",
    "    warmup_steps=1_000, # Number of warmup steps for learning rate scheduling\n",
    "    #logging_dir='./logs', # Directory to save the training logs\n",
    "    #logging_steps=100, # Number of steps to log training progress\n",
    "    #overwrite_cache=True,\n",
    "    seed=42 # Set a seed for reproducibility\n",
    ")\n",
    "\n",
    "# Instantiate the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## only predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I love Paris\"\n",
    "input_ids = tokenizer.encode(sentence, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  40, 1842, 6342]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love Paris. It\\'s a beautiful city, but it\\'s also one of the most beautiful places I\\'ve ever been to.\"\\n\\n\"I\\'m not sure if I\\'ll ever be able to live in Paris again,\" he added. \"I don\\'t know what I\\'m going to do with my life.\"<|endoftext|>'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [pipenv: PyEnv]",
   "language": "python",
   "name": "pyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
