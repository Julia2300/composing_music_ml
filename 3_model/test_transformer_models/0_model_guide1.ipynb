{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json', 'r') as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = []\n",
    "for song in data:\n",
    "    token_list.append(data[song])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = [len(song) for song in token_list]\n",
    "min(lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(lengths, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dictionary = {}\n",
    "        self.reverse_dictionary = {}\n",
    "\n",
    "        # Add the padding token\n",
    "        self.__add_to_dict('<pad>')\n",
    "\n",
    "        # Add characters and numbers to the dictionary\n",
    "        for i in range(10):\n",
    "            self.__add_to_dict(str(i))\n",
    "        for i in range(26):\n",
    "            self.__add_to_dict(chr(ord('a') + i))\n",
    "\n",
    "        # Add space and punctuation to the dictionary\n",
    "        self.__add_to_dict('.')\n",
    "        self.__add_to_dict(' ')\n",
    "\n",
    "    def __add_to_dict(self, character):\n",
    "        if character not in self.dictionary:\n",
    "            self.dictionary[character] = len(self.dictionary)\n",
    "            self.reverse_dictionary[self.dictionary[character]] = character\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return [self.dictionary[c] for c in text]\n",
    "\n",
    "    def character_to_token(self, character):\n",
    "        return self.dictionary[character]\n",
    "\n",
    "    def token_to_character(self, token):\n",
    "        return self.reverse_dictionary[token]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module that converts tokens into embeddings.\n",
    "\n",
    "    Input dimension is: (batch_size, sequence_length)\n",
    "    Output dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dimension,\n",
    "            number_of_tokens\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = torch.nn.Embedding(\n",
    "            num_embeddings=number_of_tokens,\n",
    "            embedding_dim=embedding_dimension\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module that creates a positional embedding with the same dimensions as the token embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dimension, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.positional_encoding = self.create_positional_encoding()\n",
    "\n",
    "    def create_positional_encoding(self):\n",
    "        \"\"\"\n",
    "        Creates a positional encoding matrix of size (max_sequence_length, embedding_dimension)\n",
    "        \"\"\"\n",
    "        positional_encoding = np.zeros((self.max_sequence_length, self.embedding_dimension))\n",
    "        for pos in range(self.max_sequence_length):\n",
    "            for i in range(0, self.embedding_dimension, 2):\n",
    "                positional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / self.embedding_dimension)))\n",
    "                positional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i + 1)) / self.embedding_dimension)))\n",
    "        return torch.from_numpy(positional_encoding).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Adds the positional encoding to the token embeddings.\n",
    "        \"\"\"\n",
    "        return x + self.positional_encoding[:x.size(0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSelfAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for a self attention layer.\n",
    "    This layer is used in the MultiHeadedSelfAttention module.\n",
    "\n",
    "    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "    Output dimension is: (batch_size, sequence_length, head_dimension)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dimension, head_dimension):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.head_dimension = head_dimension\n",
    "        self.query_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
    "        self.key_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
    "        self.value_layer = torch.nn.Linear(embedding_dimension, self.head_dimension)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Compute the self attention.\n",
    "\n",
    "        x dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "        output dimension is: (batch_size, sequence_length, head_dimension)\n",
    "        mask dimension is: (batch_size, sequence_length)\n",
    "\n",
    "        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n",
    "        \"\"\"\n",
    "\n",
    "        # x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        # query, key, value dimensions are: (batch_size, sequence_length, head_dimension)\n",
    "        query = self.query_layer(x)\n",
    "        key = self.key_layer(x)\n",
    "        value = self.value_layer(x)\n",
    "\n",
    "        # Calculate the attention weights.\n",
    "        # attention_weights dimensions are: (batch_size, sequence_length, sequence_length)\n",
    "        attention_weights = torch.matmul(query, key.transpose(-2, -1))\n",
    "\n",
    "        # Scale the attention weights.\n",
    "        attention_weights = attention_weights / np.sqrt(self.head_dimension)\n",
    "\n",
    "        # Apply the mask to the attention weights, by setting the masked tokens to a very low value.\n",
    "        # This will make the softmax output 0 for these values.\n",
    "        mask = mask.reshape(attention_weights.shape[0], 1, attention_weights.shape[2])\n",
    "        attention_weights = attention_weights.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Softmax makes sure all scores are between 0 and 1 and the sum of scores is 1.\n",
    "        # attention_scores dimensions are: (batch_size, sequence_length, sequence_length)\n",
    "        attention_scores = self.softmax(attention_weights)\n",
    "\n",
    "        # The attention scores are multiplied by the value\n",
    "        # Values of tokens with high attention score get highlighted because they are multiplied by a larger number,\n",
    "        # and tokens with low attention score get drowned out because they are multiplied by a smaller number.\n",
    "        # Output dimensions are: (batch_size, sequence_length, head_dimension)\n",
    "        return torch.bmm(attention_scores, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadedSelfAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for a multi head attention layer.\n",
    "\n",
    "    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "    Output dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dimension, number_of_heads):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.head_dimension = embedding_dimension // number_of_heads\n",
    "        self.number_of_heads = number_of_heads\n",
    "\n",
    "        # Create the self attention modules\n",
    "        self.self_attentions = torch.nn.ModuleList(\n",
    "            [MaskedSelfAttention(embedding_dimension, self.head_dimension) for _ in range(number_of_heads)])\n",
    "\n",
    "        # Create a linear layer to combine the outputs of the self attention modules\n",
    "        self.output_layer = torch.nn.Linear(number_of_heads * self.head_dimension, embedding_dimension)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Compute the multi head attention.\n",
    "\n",
    "        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        mask dimensions are: (batch_size, sequence_length)\n",
    "        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n",
    "        \"\"\"\n",
    "        # Compute the self attention for each head\n",
    "        # self_attention_outputs dimensions are:\n",
    "        # (number_of_heads, batch_size, sequence_length, head_dimension)\n",
    "        self_attention_outputs = [self_attention(x, mask) for self_attention in self.self_attentions]\n",
    "\n",
    "        # Concatenate the self attention outputs\n",
    "        # self_attention_outputs_concatenated dimensions are:\n",
    "        # (batch_size, sequence_length, number_of_heads * head_dimension)\n",
    "        concatenated_self_attention_outputs = torch.cat(self_attention_outputs, dim=2)\n",
    "\n",
    "        # Apply the output layer to the concatenated self attention outputs\n",
    "        # output dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        return self.output_layer(concatenated_self_attention_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for an encoder layer.\n",
    "\n",
    "    An encoder layer consists of a multi-headed self attention layer, a feed forward layer and dropout.\n",
    "\n",
    "    Input dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "    Output dimension is: (batch_size, sequence_length, embedding_dimension)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dimension,\n",
    "            number_of_heads,\n",
    "            feed_forward_dimension,\n",
    "            dropout_rate\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.number_of_heads = number_of_heads\n",
    "        self.feed_forward_dimension = feed_forward_dimension\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.multi_headed_self_attention = MaskedMultiHeadedSelfAttention(embedding_dimension, number_of_heads)\n",
    "        self.feed_forward = FeedForward(embedding_dimension, feed_forward_dimension)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.layer_normalization_1 = torch.nn.LayerNorm(embedding_dimension)\n",
    "        self.layer_normalization_2 = torch.nn.LayerNorm(embedding_dimension)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Compute the encoder layer.\n",
    "\n",
    "        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        mask dimensions are: (batch_size, sequence_length)\n",
    "        mask values are: 0 or 1. 0 means the token is masked, 1 means the token is not masked.\n",
    "        \"\"\"\n",
    "\n",
    "        # Layer normalization 1\n",
    "        normalized_x = self.layer_normalization_1(x)\n",
    "\n",
    "        # Multi headed self attention\n",
    "        attention_output = self.multi_headed_self_attention(normalized_x, mask)\n",
    "\n",
    "        # Residual output\n",
    "        residual_output = x + attention_output\n",
    "\n",
    "        # Layer normalization 2\n",
    "        normalized_residual_output = self.layer_normalization_2(residual_output)\n",
    "\n",
    "        # Feed forward\n",
    "        feed_forward_output = self.feed_forward(normalized_residual_output)\n",
    "\n",
    "        # Dropout, only when training.\n",
    "        if self.training:\n",
    "            feed_forward_output = self.dropout(feed_forward_output)\n",
    "\n",
    "        # Residual output\n",
    "        return residual_output + feed_forward_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for a stack of decoders.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embedding_dimension,\n",
    "            number_of_layers,\n",
    "            number_of_heads,\n",
    "            feed_forward_dimension,\n",
    "            dropout_rate,\n",
    "            max_sequence_length\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.number_of_layers = number_of_layers\n",
    "        self.number_of_heads = number_of_heads\n",
    "        self.feed_forward_dimension = feed_forward_dimension\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "        # Create the encoder layers\n",
    "        self.encoder_layers = torch.nn.ModuleList(\n",
    "            [DecoderLayer(embedding_dimension, number_of_heads, feed_forward_dimension, dropout_rate) for _ in\n",
    "             range(number_of_layers)])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        decoder_outputs = x\n",
    "        for decoder_layer in self.encoder_layers:\n",
    "            decoder_outputs = decoder_layer(decoder_outputs, mask)\n",
    "\n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for a feed forward layer.\n",
    "\n",
    "    A feed forward layer is a fully connected layer with a ReLU activation function in between.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dimension, feed_forward_dimension):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.feed_forward_dimension = feed_forward_dimension\n",
    "        self.linear_1 = torch.nn.Linear(embedding_dimension, feed_forward_dimension)\n",
    "        self.linear_2 = torch.nn.Linear(feed_forward_dimension, embedding_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the feed forward layer.\n",
    "        \"\"\"\n",
    "        return self.linear_2(torch.relu(self.linear_1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for a language model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            number_of_tokens,  # The number of tokens in the vocabulary\n",
    "            max_sequence_length=512,  # The maximum sequence length to use for attention\n",
    "            embedding_dimension=512,  # The dimension of the token embeddings\n",
    "            number_of_layers=6,  # The number of decoder layers to use\n",
    "            number_of_heads=4,  # The number of attention heads to use\n",
    "            feed_forward_dimension=None,  # The dimension of the feed forward layer\n",
    "            dropout_rate=0.1  # The dropout rate to use\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.number_of_tokens = number_of_tokens\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.number_of_layers = number_of_layers\n",
    "        self.number_of_heads = number_of_heads\n",
    "\n",
    "        if feed_forward_dimension is None:\n",
    "            # GPT-2 paper uses 4 * embedding_dimension for the feed forward dimension\n",
    "            self.feed_forward_dimension = embedding_dimension * 4\n",
    "        else:\n",
    "            self.feed_forward_dimension = feed_forward_dimension\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Create the token embedding layer\n",
    "        self.token_embedding = TokenEmbedding(embedding_dimension, number_of_tokens)\n",
    "\n",
    "        # Create the positional encoding layer\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dimension, max_sequence_length)\n",
    "\n",
    "        # Create the normalization layer\n",
    "        self.layer_normalization = torch.nn.LayerNorm(embedding_dimension)\n",
    "\n",
    "        # Create the decoder stack\n",
    "        self.decoder = DecoderStack(\n",
    "            embedding_dimension=embedding_dimension,\n",
    "            number_of_layers=number_of_layers,\n",
    "            number_of_heads=number_of_heads,\n",
    "            feed_forward_dimension=self.feed_forward_dimension,\n",
    "            dropout_rate=dropout_rate,\n",
    "            max_sequence_length=max_sequence_length\n",
    "        )\n",
    "\n",
    "        # Create the language model head\n",
    "        self.lm_head = LMHead(embedding_dimension, number_of_tokens)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Compute the token embeddings\n",
    "        # token_embeddings dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        token_embeddings = self.token_embedding(x)\n",
    "\n",
    "        # Compute the positional encoding\n",
    "        # positional_encoding dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        positional_encoding = self.positional_encoding(token_embeddings)\n",
    "\n",
    "        # Post embedding layer normalization\n",
    "        positional_encoding_normalized = self.layer_normalization(positional_encoding)\n",
    "\n",
    "        decoder_outputs = self.decoder(positional_encoding_normalized, mask)\n",
    "        lm_head_outputs = self.lm_head(decoder_outputs)\n",
    "\n",
    "        return lm_head_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMHead(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module for the language model head.\n",
    "    The language model head is a linear layer that maps the embedding dimension to the vocabulary size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dimension, number_of_tokens):\n",
    "        super().__init__()\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.number_of_tokens = number_of_tokens\n",
    "        self.linear = torch.nn.Linear(embedding_dimension, number_of_tokens)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the language model head.\n",
    "\n",
    "        x dimensions are: (batch_size, sequence_length, embedding_dimension)\n",
    "        output dimensions are: (batch_size, sequence_length, number_of_tokens)\n",
    "        \"\"\"\n",
    "        # Compute the linear layer\n",
    "        # linear_output dimensions are: (batch_size, sequence_length, number_of_tokens)\n",
    "        linear_output = self.linear(x)\n",
    "\n",
    "        return linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveWrapper(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch module that wraps a GPT model and makes it autoregressive.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gpt_model):\n",
    "        super().__init__()\n",
    "        self.model = gpt_model\n",
    "        self.max_sequence_length = self.model.max_sequence_length\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Autoregressive forward pass\n",
    "        \"\"\"\n",
    "        inp, target = x[:, :-1], x[:, 1:]\n",
    "        mask = mask[:, :-1]\n",
    "\n",
    "        output = self.model(inp, mask)\n",
    "        return output, target\n",
    "\n",
    "    def next_token_probabilities(self, x, mask, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Calculate the token probabilities for the next token in the sequence.\n",
    "        \"\"\"\n",
    "        logits = self.model(x, mask)[:, -1]\n",
    "\n",
    "        # Apply the temperature\n",
    "        if temperature != 1.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "        # Apply the softmax\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_sequences(max_sequence_length, tokenized_training_data):\n",
    "    # Create sequences of length max_sequence_length + 1\n",
    "    # The last token of each sequence is the target token\n",
    "    sequences = []\n",
    "    for i in range(0, len(tokenized_training_data) - max_sequence_length - 1):\n",
    "        sequences.append(tokenized_training_data[i: i + max_sequence_length + 1])\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def tokenize_and_pad_training_data(max_sequence_length, tokenizer, training_data):\n",
    "    # Tokenize the training data\n",
    "    tokenized_training_data = tokenizer.tokenize(training_data)\n",
    "    for _ in range(max_sequence_length):\n",
    "        # Prepend padding tokens\n",
    "        tokenized_training_data.insert(0, tokenizer.character_to_token('<pad>'))\n",
    "    return tokenized_training_data\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "embedding_dimension = 256\n",
    "max_sequence_length = 128\n",
    "number_of_tokens = 121\n",
    "\n",
    "# Create the model\n",
    "model = AutoregressiveWrapper(LanguageModel(\n",
    "    embedding_dimension=embedding_dimension,\n",
    "    number_of_tokens=number_of_tokens,\n",
    "    number_of_heads=4,\n",
    "    number_of_layers=3,\n",
    "    dropout_rate=0.1,\n",
    "    max_sequence_length=max_sequence_length\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, tokenizer: Tokenizer, optimizer=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        if optimizer is None:\n",
    "            self.optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "        self.tokenizer = tokenizer\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def train(self, data, epochs, batch_size):\n",
    "        loss_per_epoch = []\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "\n",
    "            # Shuffle the sequences\n",
    "            random.shuffle(data)\n",
    "\n",
    "            # Create batches of sequences and their respective mask.\n",
    "            batches = []\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                sequence_tensor = torch.tensor(data[i: i + batch_size], dtype=torch.long)\n",
    "\n",
    "                # Create the mask tensor for the batch, where 1 means the token is not a padding token\n",
    "                mask_tensor = torch.ones_like(sequence_tensor)\n",
    "                mask_tensor[sequence_tensor == self.tokenizer.character_to_token('<pad>')] = 0\n",
    "\n",
    "                batches.append((sequence_tensor, mask_tensor))\n",
    "\n",
    "            # Train the model on each batch\n",
    "            for batch in batches:\n",
    "                self.model.train()\n",
    "\n",
    "                # Create the input and mask tensors\n",
    "                input_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype=torch.long)\n",
    "                mask_tensor = torch.zeros((batch_size, self.model.max_sequence_length + 1), dtype=torch.long)\n",
    "\n",
    "                for i, input_entry in enumerate(batch[0]):\n",
    "                    input_tensor[i] = input_entry\n",
    "\n",
    "                for i, mask_entry in enumerate(batch[1]):\n",
    "                    mask_tensor[i] = mask_entry\n",
    "\n",
    "                # Compute the model output\n",
    "                model_output, target = self.model.forward(x=input_tensor, mask=mask_tensor)\n",
    "\n",
    "                # Compute the losses\n",
    "                # The loss is computed on the model output and the target\n",
    "                loss = self.loss_function(model_output.transpose(1, 2), target)\n",
    "\n",
    "                # Backpropagate the loss.\n",
    "                loss.backward()\n",
    "\n",
    "                # Clip the gradients. This is used to prevent exploding gradients.\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "\n",
    "                # Update the model parameters. This is done by taking a step in the direction of the gradient.\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Reset the gradients. This is done so that the gradients from the previous batch\n",
    "                # are not used in the next step.\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Append the loss to the list of losses, so that the average loss can be computed for this epoch.\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            # Print the loss\n",
    "            epoch_loss = np.average(losses)\n",
    "            loss_per_epoch.append(epoch_loss)\n",
    "            print('Epoch:', epoch, 'Loss:', epoch_loss)\n",
    "\n",
    "        return loss_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_left(sequence, final_length, padding_token):\n",
    "    return [padding_token] * (final_length - len(sequence)) + sequence\n",
    "\n",
    "\n",
    "class Generator:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            tokenizer):\n",
    "        self.model = model\n",
    "        #self.tokenizer = tokenizer\n",
    "\n",
    "    def generate(\n",
    "            self,\n",
    "            max_tokens_to_generate: int,\n",
    "            prompt: str = None,\n",
    "            temperature: float = 1.0,\n",
    "            eos_token: int = None,\n",
    "            padding_token: int = 0):\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        if prompt is None:\n",
    "            start_tokens = [120]\n",
    "        else:\n",
    "            start_tokens = prompt\n",
    "\n",
    "        input_tensor = torch.tensor(\n",
    "            pad_left(\n",
    "                sequence=start_tokens,\n",
    "                final_length=self.model.max_sequence_length + 1,\n",
    "                padding_token=padding_token\n",
    "            ),\n",
    "            dtype=torch.long\n",
    "        )\n",
    "\n",
    "        num_dims = len(input_tensor.shape)\n",
    "\n",
    "        if num_dims == 1:\n",
    "            input_tensor = input_tensor[None, :]\n",
    "\n",
    "        out = input_tensor\n",
    "        for _ in range(max_tokens_to_generate):\n",
    "\n",
    "            x = out[:, -self.model.max_sequence_length:]\n",
    "\n",
    "            mask = torch.ones_like(x)\n",
    "            mask[x == padding_token] = 0\n",
    "\n",
    "            # Compute the next token probabilities\n",
    "            next_token_probabilities = self.model.next_token_probabilities(\n",
    "                x=x,\n",
    "                temperature=temperature,\n",
    "                mask=mask\n",
    "            )\n",
    "\n",
    "            # Sample the next token from the probability distribution\n",
    "            next_token = torch.multinomial(next_token_probabilities, num_samples=1)\n",
    "\n",
    "            # Append the next token to the output\n",
    "            out = torch.cat([out, next_token], dim=1)\n",
    "\n",
    "            # If the end of sequence token is reached, stop generating tokens\n",
    "            if eos_token is not None and next_token == eos_token:\n",
    "                break\n",
    "\n",
    "        generated_tokens = out[0].tolist()\n",
    "        #return ''.join([self.tokenizer.token_to_character(token) for token in generated_tokens])\n",
    "        return generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "training_data = '. '.join([\n",
    "    'cats rule the world',\n",
    "    'dogs are the best',\n",
    "    'elephants have long trunks',\n",
    "    'monkeys like bananas',\n",
    "    'pandas eat bamboo',\n",
    "    'tigers are dangerous',\n",
    "    'zebras have stripes',\n",
    "    'lions are the kings of the savannah',\n",
    "    'giraffes have long necks',\n",
    "    'hippos are big and scary',\n",
    "    'rhinos have horns',\n",
    "    'penguins live in the arctic',\n",
    "    'polar bears are white'\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.6269109975546598\n",
      "Epoch: 1 Loss: 0.574933186173439\n",
      "Epoch: 2 Loss: 0.518666666932404\n",
      "Epoch: 3 Loss: 0.4665888287127018\n",
      "Epoch: 4 Loss: 0.41852995194494724\n",
      "Epoch: 5 Loss: 0.3721507368609309\n",
      "Epoch: 6 Loss: 0.32767093926668167\n",
      "Epoch: 7 Loss: 0.293087022844702\n",
      "Epoch: 8 Loss: 0.26530141616240144\n",
      "Epoch: 9 Loss: 0.2332085222005844\n",
      "Epoch: 10 Loss: 0.20160000678151846\n",
      "Epoch: 11 Loss: 0.18124062987044454\n",
      "Epoch: 12 Loss: 0.16243038745597005\n",
      "Epoch: 13 Loss: 0.13985720975324512\n",
      "Epoch: 14 Loss: 0.12673934502527118\n",
      "Epoch: 15 Loss: 0.11625302955508232\n",
      "Epoch: 16 Loss: 0.10446816170588136\n",
      "Epoch: 17 Loss: 0.0885671831201762\n",
      "Epoch: 18 Loss: 0.08252928359434009\n",
      "Epoch: 19 Loss: 0.07406864268705249\n",
      "Epoch: 20 Loss: 0.0674691058229655\n",
      "Epoch: 21 Loss: 0.06339816865511239\n",
      "Epoch: 22 Loss: 0.06056331645231694\n",
      "Epoch: 23 Loss: 0.05549391754902899\n",
      "Epoch: 24 Loss: 0.044843315496109426\n",
      "Epoch: 25 Loss: 0.0481273754267022\n",
      "Epoch: 26 Loss: 0.044788877828978\n",
      "Epoch: 27 Loss: 0.03854639397468418\n",
      "Epoch: 28 Loss: 0.03727738687302917\n",
      "Epoch: 29 Loss: 0.04335649043787271\n",
      "Epoch: 30 Loss: 0.034136687754653394\n",
      "Epoch: 31 Loss: 0.0339539151173085\n",
      "Epoch: 32 Loss: 0.028910018503665924\n",
      "Epoch: 33 Loss: 0.02910093974787742\n",
      "Epoch: 34 Loss: 0.0284699450712651\n",
      "Epoch: 35 Loss: 0.027774689195211977\n",
      "Epoch: 36 Loss: 0.02808453858597204\n",
      "Epoch: 37 Loss: 0.024890935979783535\n",
      "Epoch: 38 Loss: 0.030398572387639433\n",
      "Epoch: 39 Loss: 0.02026987448334694\n",
      "Epoch: 40 Loss: 0.02348929044092074\n",
      "Epoch: 41 Loss: 0.020723364839795977\n",
      "Epoch: 42 Loss: 0.021725714148487896\n",
      "Epoch: 43 Loss: 0.020342755888123065\n",
      "Epoch: 44 Loss: 0.01993895834311843\n",
      "Epoch: 45 Loss: 0.017020413753925823\n",
      "Epoch: 46 Loss: 0.01683063874952495\n",
      "Epoch: 47 Loss: 0.01590281690005213\n",
      "Epoch: 48 Loss: 0.017106682527810335\n",
      "Epoch: 49 Loss: 0.01656902485410683\n"
     ]
    }
   ],
   "source": [
    "# Create the training data\n",
    "training_data = '. '.join([\n",
    "    'cats rule the world',\n",
    "    'dogs are the best',\n",
    "    'elephants have long trunks',\n",
    "    'monkeys like bananas',\n",
    "    'pandas eat bamboo',\n",
    "    'tigers are dangerous',\n",
    "    'zebras have stripes',\n",
    "    'lions are the kings of the savannah',\n",
    "    'giraffes have long necks',\n",
    "    'hippos are big and scary',\n",
    "    'rhinos have horns',\n",
    "    'penguins live in the arctic',\n",
    "    'polar bears are white'\n",
    "])\n",
    "\n",
    "\"\"\"\n",
    "# Tokenize the training data\n",
    "tokenized_training_data = tokenizer.tokenize(training_data)\n",
    "\n",
    "# Add padding to the left, to make sure all parts of the sequence are being trained\n",
    "for _ in range(max_sequence_length):\n",
    "    # Prepend padding tokens\n",
    "    tokenized_training_data.insert(0, tokenizer.character_to_token('<pad>'))\n",
    "\"\"\"\n",
    "\n",
    "#tokenized_and_padded_training_data = tokenize_and_pad_training_data(max_sequence_length, tokenizer, training_data)\n",
    "#sequences = create_training_sequences(max_sequence_length, tokenized_and_padded_training_data)\n",
    "\n",
    "# Train the model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "trainer = Trainer(model, tokenizer, optimizer)\n",
    "loss_per_epoch = trainer.train(sequences, epochs=50, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ab hier für Musik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 256\n",
    "max_sequence_length = 128\n",
    "number_of_tokens = 121\n",
    "\n",
    "# Create the model\n",
    "model = AutoregressiveWrapper(LanguageModel(\n",
    "    embedding_dimension=embedding_dimension,\n",
    "    number_of_tokens=number_of_tokens,\n",
    "    number_of_heads=4,\n",
    "    number_of_layers=3,\n",
    "    dropout_rate=0.1,\n",
    "    max_sequence_length=max_sequence_length\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [ [120] + song[:max_sequence_length] for song in token_list ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 4.603653975895473\n",
      "Epoch: 1 Loss: 3.519871950149536\n",
      "Epoch: 2 Loss: 3.0146497317722867\n",
      "Epoch: 3 Loss: 2.6324515513011386\n",
      "Epoch: 4 Loss: 2.3604329994746616\n",
      "Epoch: 5 Loss: 2.157393600259508\n",
      "Epoch: 6 Loss: 2.0098560452461243\n",
      "Epoch: 7 Loss: 1.9010136127471924\n",
      "Epoch: 8 Loss: 1.8207486016409737\n",
      "Epoch: 9 Loss: 1.7591780253819056\n",
      "Epoch: 10 Loss: 1.7070641347340174\n",
      "Epoch: 11 Loss: 1.6606709786823817\n",
      "Epoch: 12 Loss: 1.6245412698813848\n",
      "Epoch: 13 Loss: 1.5981905928679876\n",
      "Epoch: 14 Loss: 1.5767116716929845\n",
      "Epoch: 15 Loss: 1.5574599632195063\n",
      "Epoch: 16 Loss: 1.5419460918222154\n",
      "Epoch: 17 Loss: 1.5288148096629552\n",
      "Epoch: 18 Loss: 1.514701519693647\n",
      "Epoch: 19 Loss: 1.5043810989175523\n",
      "Epoch: 20 Loss: 1.4931631003107344\n",
      "Epoch: 21 Loss: 1.4830221235752106\n",
      "Epoch: 22 Loss: 1.473379863159997\n",
      "Epoch: 23 Loss: 1.464551819222314\n",
      "Epoch: 24 Loss: 1.4553556314536504\n",
      "Epoch: 25 Loss: 1.447263947554997\n",
      "Epoch: 26 Loss: 1.4391339123249054\n",
      "Epoch: 27 Loss: 1.4322861731052399\n",
      "Epoch: 28 Loss: 1.425113673721041\n",
      "Epoch: 29 Loss: 1.4187962668282645\n",
      "Epoch: 30 Loss: 1.4126337383474623\n",
      "Epoch: 31 Loss: 1.4065432761396681\n",
      "Epoch: 32 Loss: 1.4015214698655265\n",
      "Epoch: 33 Loss: 1.3953539729118347\n",
      "Epoch: 34 Loss: 1.3905683244977678\n",
      "Epoch: 35 Loss: 1.3857589619500297\n",
      "Epoch: 36 Loss: 1.380002805164882\n",
      "Epoch: 37 Loss: 1.3740713255746024\n",
      "Epoch: 38 Loss: 1.3695216136319297\n",
      "Epoch: 39 Loss: 1.3638283142021723\n",
      "Epoch: 40 Loss: 1.3565539164202554\n",
      "Epoch: 41 Loss: 1.350193875176566\n",
      "Epoch: 42 Loss: 1.3427176092352187\n",
      "Epoch: 43 Loss: 1.3347918604101454\n",
      "Epoch: 44 Loss: 1.3233707717486791\n",
      "Epoch: 45 Loss: 1.3145571989672524\n",
      "Epoch: 46 Loss: 1.3033669888973236\n",
      "Epoch: 47 Loss: 1.292040420430047\n",
      "Epoch: 48 Loss: 1.279694778578622\n",
      "Epoch: 49 Loss: 1.268056299005236\n",
      "Epoch: 50 Loss: 1.2563306433813912\n",
      "Epoch: 51 Loss: 1.2427061200141907\n",
      "Epoch: 52 Loss: 1.227771861212594\n",
      "Epoch: 53 Loss: 1.2113974477563585\n",
      "Epoch: 54 Loss: 1.1944474067006792\n",
      "Epoch: 55 Loss: 1.1725746095180511\n",
      "Epoch: 56 Loss: 1.1493034022195\n",
      "Epoch: 57 Loss: 1.1206811368465424\n",
      "Epoch: 58 Loss: 1.085246205329895\n",
      "Epoch: 59 Loss: 1.0428013503551483\n",
      "Epoch: 60 Loss: 0.989087952034814\n",
      "Epoch: 61 Loss: 0.9304836307253156\n",
      "Epoch: 62 Loss: 0.8631017868007932\n",
      "Epoch: 63 Loss: 0.7872311621904373\n",
      "Epoch: 64 Loss: 0.7050401036228452\n",
      "Epoch: 65 Loss: 0.6152628830501011\n",
      "Epoch: 66 Loss: 0.5205825354371753\n",
      "Epoch: 67 Loss: 0.4367256207125528\n",
      "Epoch: 68 Loss: 0.36354087399584906\n",
      "Epoch: 69 Loss: 0.30679150990077425\n",
      "Epoch: 70 Loss: 0.2607065641454288\n",
      "Epoch: 71 Loss: 0.22581797518900462\n",
      "Epoch: 72 Loss: 0.1981018361236368\n",
      "Epoch: 73 Loss: 0.17488136089273862\n",
      "Epoch: 74 Loss: 0.1554867167557989\n",
      "Epoch: 75 Loss: 0.1408551803656987\n",
      "Epoch: 76 Loss: 0.12881174949663027\n",
      "Epoch: 77 Loss: 0.11859582151685442\n",
      "Epoch: 78 Loss: 0.11064812168478966\n",
      "Epoch: 79 Loss: 0.10307720622846059\n",
      "Epoch: 80 Loss: 0.09709358454814979\n",
      "Epoch: 81 Loss: 0.09123116891298975\n",
      "Epoch: 82 Loss: 0.08698117892657008\n",
      "Epoch: 83 Loss: 0.08180416082697255\n",
      "Epoch: 84 Loss: 0.07719879650643893\n",
      "Epoch: 85 Loss: 0.0730826056429318\n",
      "Epoch: 86 Loss: 0.06984023377299309\n",
      "Epoch: 87 Loss: 0.06649545207619667\n",
      "Epoch: 88 Loss: 0.06372036758278098\n",
      "Epoch: 89 Loss: 0.06118613508130823\n",
      "Epoch: 90 Loss: 0.057964960379259925\n",
      "Epoch: 91 Loss: 0.05594068153628281\n",
      "Epoch: 92 Loss: 0.05319019912609032\n",
      "Epoch: 93 Loss: 0.051674332203609605\n",
      "Epoch: 94 Loss: 0.04916221063051905\n",
      "Epoch: 95 Loss: 0.04788237863353321\n",
      "Epoch: 96 Loss: 0.04537575239581721\n",
      "Epoch: 97 Loss: 0.044869923165866306\n",
      "Epoch: 98 Loss: 0.042875931332153935\n",
      "Epoch: 99 Loss: 0.041533107204096656\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "trainer = Trainer(model, tokenizer, optimizer)\n",
    "loss_per_epoch = trainer.train(sequences, epochs=100, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_per_epoch)\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "max_tokens_to_generate = 50\n",
    "generator = Generator(model, tokenizer)\n",
    "generated_text = generator.generate(\n",
    "    max_tokens_to_generate=32,\n",
    "    prompt=[0],\n",
    "    padding_token=120\n",
    ")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}