{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e22ff8f9-cea1-42a2-8792-31b451e99e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a0de7b-c342-4326-b1dd-b55e0c390cfc",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21991d5a-0ae8-4538-a265-ae220545b340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "803"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data_words.json', 'r') as fp:\n",
    "    data = json.load(fp)\n",
    "\n",
    "song_list = []\n",
    "for song in data:\n",
    "    song_list.append(data[song])\n",
    "\n",
    "len(song_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e369500-5541-4ee1-87ec-84f11d732b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bar_None Position_3/16 Note-On_76 Note-Duration_2 Position_4/16 Note-On_74 Note-Duration_2 Position_'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_sentences = [\" \".join(song) for song in song_list]\n",
    "song_sentences[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6327f311-3851-4e98-a3de-0571bb8d7bad",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "665630ba-9ee6-44d3-a88c-a8175d44f8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#device = \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0b984b6-7c64-4f76-bfa8-48e2f715494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer(\n",
    "    vocab_file=\"vocab.json\", \n",
    "    merges_file=\"merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ab9c264-c26a-4fdc-8041-5472c52006e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd418e89-c5b5-4785-9802-9eb0d74964ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': 'PAD'})\n",
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87c75f54-132a-47d8-91ed-1d5c94d5a29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data length: 803\n",
      "90% at:      722\n"
     ]
    }
   ],
   "source": [
    "split_train_test = int(0.9*len(song_sentences))\n",
    "print(\"data length:\", len(song_sentences))\n",
    "print(\"90% at:     \", split_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5af33640-24c0-4470-87dd-377f93903bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = song_sentences[:split_train_test]\n",
    "eval_data = song_sentences[split_train_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c47d3a50-0167-41b9-a77d-c8c496f0a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetNew(Dataset):\n",
    "    def __init__(self, tokenizer, data, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            self.data[index].split(\" \"),\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100  # Set padding tokens to -100 for language modeling\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "560c0bf4-dd12-47aa-9c15-316c55f66e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your custom Dataset\n",
    "train_dataset = CustomDatasetNew(tokenizer=tokenizer, data=train_data, max_length=32)\n",
    "eval_dataset = CustomDatasetNew(tokenizer=tokenizer, data=eval_data, max_length=32)\n",
    "\n",
    "# Define your data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9b20777-9d4a-46ce-b247-dbe52752d798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  0, 107, 118,   5,  38, 109,   8,  38, 111, 118,  10,  38, 113,  13,\n",
       "           44, 101, 115, 118,  12,  52, 101,   0, 103, 118,  10,  38, 105,   8,\n",
       "           38, 107, 118,  10]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'labels': tensor([[  0, 107, 118,   5,  38, 109,   8,  38, 111, 118,  10,  38, 113,  13,\n",
       "           44, 101, 115, 118,  12,  52, 101,   0, 103, 118,  10,  38, 105,   8,\n",
       "           38, 107, 118,  10]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52807e76-37dc-41e6-91c2-ebff97c93534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GPT-2 model architecture\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=512, # max seq length\n",
    "    n_embd=32,\n",
    "    n_head=2, \n",
    "    n_layer=3,\n",
    "    dropout=0.1 \n",
    ")\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28f71513-e8f2-4985-9ea2-d89d3140386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"out\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=4, # You can adjust the batch size per device as needed\n",
    "    save_steps=1000,\n",
    "    save_total_limit=5, # maximum number of models to save\n",
    "    learning_rate=1e-4, # You can adjust the learning rate as needed\n",
    "    #weight_decay=0.01, # You can adjust the weight decay as needed\n",
    "    #warmup_steps=1_000, # Number of warmup steps for learning rate scheduling\n",
    "    logging_dir='logs', # Directory to save the training logs\n",
    "    logging_steps=100, # Number of steps to log training progress\n",
    "    seed=4711, # Set a seed for reproducibility\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Create and train  Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f36c7e8a-36cb-4317-9e7c-0314f6477d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jlauten_ext/PyEnv/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1810' max='1810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1810/1810 00:34, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.407200</td>\n",
       "      <td>4.123715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.915200</td>\n",
       "      <td>3.762995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.613000</td>\n",
       "      <td>3.505984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.381400</td>\n",
       "      <td>3.298367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.198600</td>\n",
       "      <td>3.139761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.059800</td>\n",
       "      <td>3.025322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.961900</td>\n",
       "      <td>2.946981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.895400</td>\n",
       "      <td>2.892172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.851700</td>\n",
       "      <td>2.862579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.831100</td>\n",
       "      <td>2.852664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "training = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6e05523-641e-4d5f-9d1f-95526efd8a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1810, training_loss=3.311536824769078, metrics={'train_runtime': 34.781, 'train_samples_per_second': 207.585, 'train_steps_per_second': 52.04, 'total_flos': 52921098240.0, 'train_loss': 3.311536824769078, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3979eff4-dd78-4c23-89bc-b384345a7207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"Bar_None\".split(\" \"), return_tensors=\"pt\")\n",
    "inputs = inputs.to(device)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e313a9b-1d8f-4bd6-bdfa-9378e1a62036",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Variante A ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f6a7a73-af18-41e0-af16-13b92be730eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 120])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(inputs)\n",
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85095d2a-ed8f-4343-8e54-522c848fe4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8716, -0.8818, -1.2642, -0.9056, -1.1093, -0.4502, -0.9754,\n",
       "          -1.2616, -0.1503, -1.2166, -0.1434, -1.4028, -0.4292, -0.1614,\n",
       "          -1.2045, -0.2191, -1.3437, -0.1735, -1.0774, -1.2823, -0.4968,\n",
       "          -1.2509, -0.7181, -1.1255, -0.9473, -0.8294, -1.2645, -0.9667,\n",
       "          -1.2132, -1.1592, -1.2247, -1.3238, -1.1587, -1.3723, -1.3077,\n",
       "          -1.3144, -1.3356, -1.2003,  0.3147, -1.1871,  0.2662, -1.3798,\n",
       "          -0.4691, -1.3352,  0.1222, -1.3819, -0.8457, -1.2799, -0.8460,\n",
       "          -1.2346, -1.0648, -1.1097, -0.5477, -1.1242, -1.2166, -1.3411,\n",
       "          -1.1694, -1.2744, -1.0797, -1.3393, -1.1808, -1.2164, -1.1597,\n",
       "          -1.2358, -1.2140, -1.4151, -1.1245, -1.1122, -0.9653, -1.2103,\n",
       "          -1.2045, -1.1862, -1.2442, -1.3524, -1.3124, -1.3874, -1.1369,\n",
       "          -1.2478, -1.4003, -1.3019, -1.2208, -1.4746, -1.3512, -1.4884,\n",
       "          -1.1901, -1.4030, -1.3098, -1.3627, -1.2748, -1.1804, -1.2501,\n",
       "          -1.2360, -1.1162, -1.4035, -1.1791, -1.4242, -1.2378, -1.1853,\n",
       "          -1.4006, -1.4046, -1.0889,  1.3781,  1.5719,  1.8352,  1.6656,\n",
       "           1.1335,  1.3461,  1.7061,  1.5523,  1.0998,  1.4116,  1.7006,\n",
       "           1.6806,  0.8106,  1.3951,  1.6230,  1.4983,  1.0098,  0.5298,\n",
       "           0.1119]]], device='cuda:0', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61b43a14-762f-4f18-81de-a5fb710887a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[23]], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Temperature value\n",
    "temperature = 13\n",
    "\n",
    "# Convert logits to probabilities using softmax with temperature\n",
    "probs = F.softmax(outputs.logits / temperature, dim=-1)\n",
    "\n",
    "# Sample a token from the probability distribution for each position in the sequence\n",
    "predicted_tokens = torch.multinomial(probs.view(-1, probs.shape[-1]), num_samples=1).view(*probs.shape[:-1])\n",
    "predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bba3c96-7bf2-4fef-9ebe-0275f64a5bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Note-On_82'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(predicted_tokens[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa891493-c9a9-4e56-99d5-b54d65ceb336",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Variante B ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "578a0c47-56cf-4234-b3e7-74b40c7ba585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0, 103,  17,  38,   0, 103,  17,  38,   0, 103,  17,  38,   0,\n",
       "         103,  17,  38,   0, 103,  17]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#outputs = model.generate(inputs, max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
    "outputs = model.generate(inputs, max_length=20, temperature=0.8)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb63a771-626f-443f-b9e9-921d213905f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bar_NoneBar_NonePosition_2/16Note-On_76Note-Duration_2Bar_NonePosition_2/16Note-On_76Note-Duration_2Bar_NonePosition_2/16Note-On_76Note-Duration_2Bar_NonePosition_2/16Note-On_76Note-Duration_2Bar_NonePosition_2/16Note-On_76'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b28662-9f31-4d78-9124-f2b15f3b2a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e416491a-3475-4949-b291-9962e141362f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21426bb5-d3bc-41b3-b143-7dd66081cbba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [pipenv: PyEnv]",
   "language": "python",
   "name": "pyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
